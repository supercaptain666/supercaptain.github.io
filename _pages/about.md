---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
é™ˆä¿Šæ°ï¼Œåšå£«ï¼Œè®²å¸ˆï¼Œç¡•å£«ç”Ÿå¯¼å¸ˆã€‚ 
2014-2018:å››å·å¤§å­¦ï¼Œå·¥å­¦å­¦å£«ã€‚
2018-2023ï¼šä¸Šæµ·äº¤é€šå¤§å­¦ï¼Œå·¥å­¦åšå£«ã€‚
ç ”ç©¶æ–¹å‘ï¼šè®¡ç®—æœºè§†è§‰ã€æ·±åº¦å­¦ä¹ ã€è¿ç§»å­¦ä¹ ã€å§¿æ€ä¼°è®¡ã€åˆ†å‰²ã€æ£€æµ‹ã€åˆ†ç±»ã€‚
<a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).


# ğŸ”¥ News
- *2025.02*: &nbsp;ğŸ‰ğŸ‰ one papers are accepted by CVPR 2025.
- *2024.02*: &nbsp;ğŸ‰ğŸ‰ one papers are accepted by CVPR 2024.

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/CVPR2024Framework.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Meta-Point Learning and Refining for Category-Agnostic Pose Estimation]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Meta-Point_Learning_and_Refining_for_Category-Agnostic_Pose_Estimation_CVPR_2024_paper.pdf))

**Junjie chen**, Jiebin Yan, Yuming Fang, and Li Niu

[**Project**](https://github.com/chenbys/MetaPoint)   <!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- First to learn class-agnostic potential keypoints for CAPE. 
</div>
</div>
- **Junjie Chen**, Weilong Chen, Yifan Zuo, and Yuming Fang. "Recurrent Feature Mining and Keypoint Mixup Padding for Category-Agnostic Pose Estimation." `In CVPR 2025`.
- **Junjie Chen**, Li Niu, Siyuan Zhou, Jianlou Si, Chen Qian, and Liqing Zhang. "Weak-shot Semantic Segmentation via Dual Similarity Transfer." `In NeurIPS 2022`.
- **Junjie Chen**, Li Niu, Liu Liu, and Liqing Zhang. "Weak-shot fine-grained classification via similarity transfer." `In NeurIPS 2021`.
- **Junjie Chen**, Li Niu, and Liqing Zhang. "Depth Privileged Scene Recognition via Dual Attention Hallucination." `IEEE Trans. Image Process. 30 (2021): 9164-9178`.
- **Junjie Chen**, Li Niu, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. "Amodal Instance Segmentation via Prior-guided Expansion." `In AAAI 2023`.
- Yi Tu, Li Niu, **Junjie Chen**, Dawei Cheng, and Liqing Zhang. "Learning from web data with self-organizing memory module." `In CVPR 2020`.
- Yan Liu, Zhijie Zhang, Li Niu, **Junjie Chen**, and Liqing Zhang. "Mixed supervised object detection by transferring mask prior and semantic similarity." `In NeurIPS 2021`.
- Jieteng Yao, **Junjie Chen**, Li Niu, Bin Sheng. Scene-aware Human Pose Generation using Transformer. `In ACM MM 2023`.
- Zhijie Zhang, Yan Liu, **Junjie Chen**, Li Niu, and Liqing Zhang. "Depth Privileged Object Detection in Indoor Scenes via Deformation Hallucination." `In AAAI 2021`.
- Jiangtong Li, Wentao Wang, **Junjie Chen**, Li Niu, Jianlou Si, Chen Qian, and Liqing Zhang. "Video Semantic Segmentation via Sparse Temporal Transformer." `In ACM MM 2021`.


# ğŸ– Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ“– Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
